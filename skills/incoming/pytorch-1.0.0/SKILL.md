---
name: PyTorch
description: Avoid common PyTorch mistakes â€” train/eval mode, gradient leaks, device mismatches, and checkpoint gotchas.
metadata: {"clawdbot":{"emoji":"ðŸ”¥","requires":{"bins":["python3"]},"os":["linux","darwin","win32"]}}
---

## Train vs Eval Mode
- `model.train()` enables dropout, BatchNorm updates â€” default after init
- `model.eval()` disables dropout, uses running stats â€” MUST call for inference
- Mode is sticky â€” train/eval persists until explicitly changed
- `model.eval()` doesn't disable gradients â€” still need `torch.no_grad()`

## Gradient Control
- `torch.no_grad()` for inference â€” reduces memory, speeds up computation
- `loss.backward()` accumulates gradients â€” call `optimizer.zero_grad()` before backward
- `zero_grad()` placement matters â€” before forward pass, not after backward
- `.detach()` to stop gradient flow â€” prevents memory leak in logging

## Device Management
- Model AND data must be on same device â€” `model.to(device)` and `tensor.to(device)`
- `.cuda()` vs `.to('cuda')` â€” both work, `.to(device)` more flexible
- CUDA tensors can't convert to numpy directly â€” `.cpu().numpy()` required
- `torch.device('cuda' if torch.cuda.is_available() else 'cpu')` â€” portable code

## DataLoader
- `num_workers > 0` uses multiprocessing â€” Windows needs `if __name__ == '__main__':`
- `pin_memory=True` with CUDA â€” faster transfer to GPU
- Workers don't share state â€” random seeds differ per worker, set in `worker_init_fn`
- Large `num_workers` can cause memory issues â€” start with 2-4, increase if CPU-bound

## Saving and Loading
- `torch.save(model.state_dict(), path)` â€” recommended, saves only weights
- Loading: create model first, then `model.load_state_dict(torch.load(path))`
- `map_location` for cross-device â€” `torch.load(path, map_location='cpu')` if saved on GPU
- Saving whole model pickles code path â€” breaks if code changes

## In-place Operations
- In-place ops end with `_` â€” `tensor.add_(1)` vs `tensor.add(1)`
- In-place on leaf variable breaks autograd â€” error about modified leaf
- In-place on intermediate can corrupt gradient â€” avoid in computation graph
- `tensor.data` bypasses autograd â€” legacy, prefer `.detach()` for safety

## Memory Management
- Accumulated tensors leak memory â€” `.detach()` logged metrics
- `torch.cuda.empty_cache()` releases cached memory â€” but doesn't fix leaks
- Delete references and call `gc.collect()` â€” before empty_cache if needed
- `with torch.no_grad():` prevents graph storage â€” crucial for validation loop

## Common Mistakes
- BatchNorm with `batch_size=1` fails in train mode â€” use eval mode or `track_running_stats=False`
- Loss function reduction default is 'mean' â€” may want 'sum' for gradient accumulation
- `cross_entropy` expects logits â€” not softmax output
- `.item()` to get Python scalar â€” `.numpy()` or `[0]` deprecated/error
